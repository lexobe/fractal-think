"""
ç®€å•æµ‹è¯• - éªŒè¯åŸºæœ¬åŠŸèƒ½
"""

import asyncio
import pytest

# å¯¼å…¥åœ¨pytestå’Œç›´æ¥è¿è¡Œæ—¶éƒ½èƒ½å·¥ä½œ
try:
    from src.fractal_think import solve_async, ExecutionBudget, SolveStatus, S, SolveResult, TokenUsage, ExecutionFrame
    from src.fractal_think.examples.mock_operators import AsyncMockThinkLLM, AsyncMockEvalLLM
except ImportError:
    import sys
    import os
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from src.fractal_think import solve_async, ExecutionBudget, SolveStatus, S, SolveResult, TokenUsage, ExecutionFrame
    from src.fractal_think.examples.mock_operators import AsyncMockThinkLLM, AsyncMockEvalLLM


def test_basic_imports():
    """æµ‹è¯•åŸºæœ¬å¯¼å…¥åŠŸèƒ½"""
    # åˆ›å»ºåŸºæœ¬å¯¹è±¡
    node = S(goal="æµ‹è¯•ç›®æ ‡")
    assert node.goal == "æµ‹è¯•ç›®æ ‡"
    assert node.level == 0
    assert len(node.done) == 0


@pytest.mark.asyncio
async def test_basic_async_solve():
    """è¿è¡ŒåŸºæœ¬å¼‚æ­¥æµ‹è¯•"""
    think_llm = AsyncMockThinkLLM(simulation_delay=0.01, verbose=False)
    eval_llm = AsyncMockEvalLLM(simulation_delay=0.01, verbose=False)

    result = await solve_async(
        goal="ç®€å•æµ‹è¯•ä»»åŠ¡",
        think_llm=think_llm,
        eval_llm=eval_llm,
        budget=ExecutionBudget(max_depth=2, max_tokens=500, max_time=5.0)
    )

    assert result.status == SolveStatus.COMPLETED
    assert result.token_usage.total > 0
    assert think_llm.call_count > 0


@pytest.mark.asyncio
async def test_specification_example():
    """æµ‹è¯•è§„èŒƒç‰ˆç®—å­"""
    try:
        from src.fractal_think.examples.specification_operators import (
            SpecificationAIArtThink, SpecificationAIArtEval
        )
    except ImportError:
        import sys
        import os
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from src.fractal_think.examples.specification_operators import (
            SpecificationAIArtThink, SpecificationAIArtEval
        )

    think_llm = SpecificationAIArtThink(simulation_delay=0.01, verbose=False)
    eval_llm = SpecificationAIArtEval(simulation_delay=0.01, verbose=False)

    result = await solve_async(
        goal='å†™ä¸€ç¯‡"AIä¸è‰ºæœ¯"çš„çŸ­æ–‡',
        think_llm=think_llm,
        eval_llm=eval_llm,
        budget=ExecutionBudget(max_depth=3, max_tokens=1000, max_time=10.0)
    )

    assert result.status == SolveStatus.COMPLETED
    assert "AIä¸è‰ºæœ¯" in result.result
    assert len(result.result) > 100  # Updated to match exact specification output
    assert "ç”Ÿæˆè‰ºæœ¯æ®µè½å·²ç»å†™å®Œï¼Œå«DALLÂ·Eä¾‹ï¼Œå¹¶ç¬¦åˆè¦æ±‚ã€‚" in result.result  # Exact specification text
    assert "è¾…åŠ©åˆ›ä½œæ®µè½å·²ç»å†™å®Œï¼ŒåŒ…å«äº†ä¸€ä¸ªèƒ½æ‰“åŠ¨äººçš„äº‹ä¾‹ï¼Œç¬¦åˆè¦æ±‚ã€‚" in result.result  # Exact original text
    assert "è‰ºæœ¯è¯„è®ºæ®µè½å·²ç»å†™å®Œï¼Œæä¾›äº†æ·±å…¥çš„åˆ†æå’Œè§è§£ï¼Œç¬¦åˆè¦æ±‚ã€‚" in result.result  # Exact original text


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
    test_basic_imports()
    print("âœ… å¯¼å…¥æµ‹è¯•é€šè¿‡")

    async def run_tests():
        await test_basic_async_solve()
        print("âœ… åŸºæœ¬å¼‚æ­¥æµ‹è¯•é€šè¿‡")

        await test_specification_example()
        print("âœ… è§„èŒƒç¤ºä¾‹æµ‹è¯•é€šè¿‡")

    asyncio.run(run_tests())
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")